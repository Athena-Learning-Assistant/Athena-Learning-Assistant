{
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "name": "Athena PDF RAG"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "accelerator": "GPU",
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 8647788,
          "sourceType": "datasetVersion",
          "datasetId": 5179709
        }
      ],
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'paper-nurul:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F5179709%2F8647788%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240615%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240615T145927Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3Db201d4e03b7a0d726ff774e09289e65c16030d3de30f44dee3826b00b70b846712b757d8b3f0487f014cb29568ea7572404e3f6be172e367d69fb1f91605eb1959d69d4095c839c6d93a87af8eb267c97e601483a6e85bc585f58e5aa90e1a57b9062ee4b2e045949e023612489bb92f79df4551cae855926834ae887f6bae743524a1d9f791f0ea58055533306a15636b724fe6e43581859c1040c0c91d31f3363c36ffa33a2a96ba69a478216ac8cde7e11ce19c6e42254082f3b86edc101aa238521c841dde6423ab812965db34e3e7569b5667de51d2377e2305dc30d6a965fbf7b8140f8be1eaf98bfe926b2051cce227b20f235f16432f8510b4d03b6c'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "T0OeJka5aSdj"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RAG System Using Llama2 With Hugging Face\n",
        "## Import the required library\n",
        "pypdf which is used to manipulate PDF files in Python. Key functions include:\n",
        "\n",
        "* Reading PDFs: Open and read PDF content.\n",
        "* Merging PDFs: Combine multiple PDFs into one.\n",
        "* Splitting PDFs: Extract specific pages from a PDF.\n",
        "* Rotating Pages: Rotate pages within a PDF.\n",
        "* Adding Watermarks: Insert watermarks into PDF pages.\n",
        "* Filling Forms: Fill out and extract data from PDF forms.\n",
        "* Extracting Text: Extract text from PDF pages."
      ],
      "metadata": {
        "id": "tnUOq3dZeDK1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pypdf"
      ],
      "metadata": {
        "id": "tHP9Gfafdq57",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Transformers: Provides pre-trained models for NLP tasks.\n",
        "* Einops: Facilitates flexible tensor operations.\n",
        "* Accelerate: Streamlines training and inference on GPUs and TPUs.\n",
        "* Langchain: Builds applications with language models and external data.\n",
        "* Bitsandbytes: Optimizes memory usage for large model training."
      ],
      "metadata": {
        "id": "LjFiVyLXaSdo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers einops accelerate langchain bitsandbytes"
      ],
      "metadata": {
        "id": "YYeHrwmHfbgB",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install sentence transformer for generating sentence embeddings. Sentence embeddings are numerical representations of sentences that capture their semantic meaning, allowing similarity comparisons between sentences based on their content rather than just their lexical overlap."
      ],
      "metadata": {
        "id": "X71q5D5faSdo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Embedding\n",
        "!pip install install sentence_transformers"
      ],
      "metadata": {
        "id": "Tu1g-xwnfv9s",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "installs the Python package llama_index, which provides functionality for building and querying large-scale approximate nearest neighbor (ANN) indexes. These indexes are essential for efficiently searching through high-dimensional data, such as embeddings from natural language processing models.\n",
        "\n",
        "Key features of llama_index typically include:\n",
        "\n",
        "Building Indexes: Allows you to construct ANN indexes from your dataset.\n",
        "\n",
        "Querying Indexes: Supports fast retrieval of nearest neighbors based on cosine similarity or other distance metrics.\n",
        "\n",
        "Efficiency: Designed to handle large datasets efficiently, making it suitable for production systems and applications where speed and scalability are critical."
      ],
      "metadata": {
        "id": "PVYl3qybaSdp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install llama_index"
      ],
      "metadata": {
        "trusted": true,
        "id": "x__c_1ITaSdp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install llama-index-llms-huggingface"
      ],
      "metadata": {
        "trusted": true,
        "id": "1jxU6RFbaSdq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the most important library for the RAG system! llama index can give us some extra feature to store pdfs as vector index, making pipelines, etc"
      ],
      "metadata": {
        "id": "DnwsdKUyaSdq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import VectorStoreIndex,SimpleDirectoryReader,ServiceContext\n",
        "from llama_index.llms.huggingface import HuggingFaceLLM\n",
        "from llama_index.core.prompts.prompts import SimpleInputPrompt"
      ],
      "metadata": {
        "id": "HutpiHgpgLBE",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load your data files (you can add more than 1)\n",
        "with llama index, we can read the directory or files (pdfs) into text chunks"
      ],
      "metadata": {
        "id": "0g61u4CGaSdq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "documents=SimpleDirectoryReader(\"/kaggle/input/paper-nurul/\").load_data()\n",
        "documents"
      ],
      "metadata": {
        "id": "KcU-nQ_XgxWD",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "System Prompt: This is a structured text snippet used to integrate with systems like LLama2. It provides instructions to the system about its role or task. In this case, the prompt defines the system as an assistant teacher whose responsibility is to provide accurate answers to questions and fulfill requests from the teacher based on the given context and instructions. It also specifies that if the system doesn't have the answer, it should acknowledge its knowledge limitation.\n",
        "\n",
        "Query Wrapper Prompt: This represents a standard format for requesting input from users or systems regarding questions or requests. Using SimpleInputPrompt, the system can receive input in the form of questions or specific instructions that it needs to address.\n",
        "\n",
        "By using these prompts, LLama2 can effectively function as an assistant teacher, responding accurately to queries and fulfilling tasks based on the provided context and instructions."
      ],
      "metadata": {
        "id": "39BSAxbCaSdr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt=\"\"\"\n",
        "Kamu adalah seorang asisten guru. tugas kamu adalah memberikan semua jawaban dari pertanyaan yang ditanyakan, dan permintaan yang diminta oleh guru seakurat mungkin berdasarkan instruksi dan konteks yang diberikan. Jika kamu tidak tahu jawabannya, bilang kamu tidak mengetahui hal tersebut karena keterbatasan pengetahuan.\n",
        "\"\"\"\n",
        "## Default format supportable by LLama2\n",
        "query_wrapper_prompt=SimpleInputPrompt(\"<|USER|>{query_str}<|ASSISTANT|>\")"
      ],
      "metadata": {
        "id": "2g7M89fKg54H",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "if model that you stored in huggingface need permission, login into your account. but for us, the model is free to use, so we can skip this one!"
      ],
      "metadata": {
        "id": "KKN8MkZlaSdr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "S5-mOhoIhfo0",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Instance set up\n",
        "**Context Parameters:**\n",
        "context_window=4096: Specifies the maximum context length (in tokens) the model considers, which helps in maintaining relevant conversational context.\n",
        "\n",
        "**Generation Parameters:**\n",
        "max_new_tokens=512: Limits the number of tokens the model can generate in response to a query, ensuring concise and relevant answers.\n",
        "generate_kwargs={\"temperature\": 0.5, \"do_sample\": False}: Controls the generation process. temperature influences the randomness of responses, while do_sample=False suggests deterministic output.\n",
        "\n",
        "**Prompt Settings:**\n",
        "system_prompt=system_prompt: Provides the system prompt text, defining the assistant's role and behavior.\n",
        "query_wrapper_prompt=query_wrapper_prompt: Defines the format for user queries or instructions.\n",
        "\n",
        "**Model and Tokenizer:**\n",
        "tokenizer_name=\"Equinox391/Athena-Llama-2-7b-chat-finetune\": Specifies the tokenizer used with the model.\n",
        "model_name=\"Equinox391/Athena-Llama-2-7b-chat-finetune\": Indicates the fine-tuned model to be loaded for chat interactions.\n",
        "\n",
        "**Device and Model Optimization:**\n",
        "device_map=\"auto\": Automatically selects the appropriate device (CPU or GPU).\n",
        "model_kwargs={\"torch_dtype\": torch.float16, \"load_in_8bit\": True}: Optimizes model memory usage by loading in 8-bit format (load_in_8bit=True) and using torch.float16 for reduced precision calculations, which can be beneficial for performance on CUDA-enabled GPUs.\n",
        "This setup allows the HuggingFaceLLM instance to efficiently handle chat-based interactions using a fine-tuned large language model, ensuring both performance and resource efficiency, especially when leveraging CUDA-enabled GPUs."
      ],
      "metadata": {
        "id": "-bJvZ19IaSdr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Equinox391 is my username on huggingface, you can find out more [here](https://huggingface.co/Equinox391)"
      ],
      "metadata": {
        "id": "BIS6hxudaSdr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "llm = HuggingFaceLLM(\n",
        "    context_window=4096,\n",
        "    max_new_tokens=512,\n",
        "    generate_kwargs={\"temperature\": 0.5, \"do_sample\": False},\n",
        "    system_prompt=system_prompt,\n",
        "    query_wrapper_prompt=query_wrapper_prompt,\n",
        "    tokenizer_name=\"Equinox391/Athena-Llama-2-7b-chat-finetune\",\n",
        "    model_name=\"Equinox391/Athena-Llama-2-7b-chat-finetune\",\n",
        "    device_map=\"auto\",\n",
        "    # uncomment this if using CUDA to reduce memory usage\n",
        "    model_kwargs={\"torch_dtype\": torch.float16 , \"load_in_8bit\":True}\n",
        ")"
      ],
      "metadata": {
        "id": "jyVOhSuhhqdb",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -U langchain-community"
      ],
      "metadata": {
        "trusted": true,
        "id": "YQK9LWAXaSdr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install llama-index-embeddings-langchain"
      ],
      "metadata": {
        "trusted": true,
        "id": "D-65JRRRaSdr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting up embedding algorithm\n",
        "**HuggingFaceEmbeddings:**\n",
        "This is a class or module (HuggingFaceEmbeddings) from langchain.embeddings.huggingface. It represents embeddings generated by Hugging Face's models, which are popular for natural language processing tasks.\n",
        "\n",
        "**LangchainEmbedding:**\n",
        "This is a class (LangchainEmbedding) from llama_index.embeddings.langchain. It appears to integrate different embeddings, possibly including those from Hugging Face, into the LLama framework or indexing system (llama_index).\n",
        "\n",
        "**Initialization:**\n",
        "HuggingFaceEmbeddings(model_name=\"Hvare/Athena-indobert-finetuned-indonli-SentenceTransformer\"): Initializes Hugging Face embeddings using the specified model name (Hvare/Athena-indobert-finetuned-indonli-SentenceTransformer). This model name likely refers to a fine-tuned Indonesian language model for sentence embeddings. **Hvare is the huggingface username of Gilang Kurnia Mandari, one of our ML team member.**\n",
        "\n",
        "**Embedding Integration:**\n",
        "LangchainEmbedding(...): Incorporates the Hugging Face embeddings into the LLama framework or indexing system, allowing for seamless integration of these embeddings into larger applications or pipelines within llama_index.\n",
        "This setup enables the use of fine-tuned embeddings from Hugging Face within the LLama ecosystem, potentially for tasks such as semantic search, clustering, or other applications that require efficient handling of text embeddings."
      ],
      "metadata": {
        "id": "-A4EsJ3AaSds"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
        "# from llama_index import ServiceContext\n",
        "from llama_index.embeddings.langchain import LangchainEmbedding\n",
        "\n",
        "embed_model=LangchainEmbedding(\n",
        "    HuggingFaceEmbeddings(model_name=\"Hvare/Athena-indobert-finetuned-indonli-SentenceTransformer\"))"
      ],
      "metadata": {
        "id": "pr1EN5sViQm9",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code snippet below is for initializes a ServiceContext object using the from_defaults method. Here's how I see each part:\n",
        "\n",
        "**ServiceContext:**\n",
        "ServiceContext is a class or object within the LLama framework that manages the context and configuration for a service or application. It handles tasks such as data processing, model interactions, and response management.\n",
        "\n",
        "**from_defaults:**\n",
        "from_defaults is a method or function that I use to create a ServiceContext object with default settings. It simplifies the initialization process by providing sensible default values for parameters.\n",
        "\n",
        "**Parameters:**\n",
        "chunk_size=1024: I set the size of data chunks or batches to process at 1024. Chunking helps manage memory and computational resources efficiently, especially when dealing with large datasets.\n",
        "\n",
        "**llm:** I understand llm as a reference to a large language model instance that's already been set up earlier in my memory. This model is capable of generating responses or performing various language-related tasks.\n",
        "\n",
        "**embed_model:** This refers to an embedding model (embed_model) initialized using LangchainEmbedding with Hugging Face embeddings. This model helps encode text into numerical representations (embeddings).\n",
        "\n",
        "In summary, I initialize service_context to ensure smooth operation within the LLama framework, optimizing resource usage and leveraging models like llm and embed_model for effective text processing and response generation."
      ],
      "metadata": {
        "id": "YywZDsoKaSds"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "service_context=ServiceContext.from_defaults(\n",
        "    chunk_size=1024,\n",
        "    llm=llm,\n",
        "    embed_model=embed_model\n",
        ")"
      ],
      "metadata": {
        "id": "LduiJD2ajpy4",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "service_context"
      ],
      "metadata": {
        "id": "vQ_jtoK3kCeT",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next step is to make sure the documents that we've vectorised using VectorStoreIndex stored to an index as a new temporary knowledge for  our models"
      ],
      "metadata": {
        "id": "Ef6ejDwUaSds"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "index=VectorStoreIndex.from_documents(documents,service_context=service_context)"
      ],
      "metadata": {
        "id": "31jvrW2BkFEL",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index"
      ],
      "metadata": {
        "id": "IM-gNZ0-kRnO",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adding index to query engine"
      ],
      "metadata": {
        "id": "VeIxzH1baSds"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query_engine=index.as_query_engine()"
      ],
      "metadata": {
        "id": "uSJzrMm6kTxf",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing\n",
        "## Giving question based on pdf files, as input for the model to test it"
      ],
      "metadata": {
        "id": "oeCl9ftYaSdt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response=query_engine.query(\"Apa itu lstm?\")"
      ],
      "metadata": {
        "id": "PdhKFWCTkZRx",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## model output"
      ],
      "metadata": {
        "id": "-AOIQF8IaSdt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(response)"
      ],
      "metadata": {
        "id": "XKsLsgWSkfGD",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}